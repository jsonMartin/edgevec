# W8.1 Context Bundle - Architecture & Data Layout Excerpts

**Purpose:** Consolidated reference material for Binary Quantization implementation
**Target Agent:** RUST_ENGINEER
**Last Updated:** 2025-12-11

---

## ARCHITECTURE EXCERPTS

### From `docs/architecture/ARCHITECTURE.md` Section 4.2: Quantization

**Performance Requirements:**
```
R4: <10ms search for 100k vectors (P99)
R5: <100 bytes/vector memory budget
```

**Quantization Strategy:**
- **Algorithm:** Binary Quantization (sign-based)
- **Compression Ratio:** 8x (768 × 4 bytes → 96 bytes)
- **Distance Metric:** Hamming distance (bitwise XOR + popcount)
- **Target Performance:** <50 CPU cycles per Hamming comparison (x86_64)

**Memory Budget Allocation:**
```
Per Vector (768D, Binary Quantized):
- Quantized data: 96 bytes (768 bits)
- Metadata: 4 bytes (vector ID, version)
- HNSW neighbors: Variable (compressed VByte)
- Total: <100 bytes/vector (excluding neighbors)
```

**Constraints:**
- WASM-compatible: No std::arch (must cfg-gate SIMD)
- no_std friendly: Can use alloc
- Deterministic: Same input → same output, always
- SIMD-ready: 64-byte alignment for AVX2 optimization (future)

---

## DATA LAYOUT EXCERPTS

### From `docs/architecture/DATA_LAYOUT.md` Section 3: Binary Quantization

**Memory Layout:**
```
QuantizedVector:
  ├── data: [u8; 96]           // 768 bits packed
  ├── Alignment: 64 bytes       // SIMD requirement
  └── Total size: 96 bytes
```

**Bit Packing Format:**
```
Input:  [f32; 768]  (3072 bytes)
Output: [u8; 96]    (96 bytes)

Packing rule (little-endian):
  - Byte 0: bits [0..8]   (dimensions 0-7)
  - Byte 1: bits [8..16]  (dimensions 8-15)
  - ...
  - Byte 95: bits [760..768] (dimensions 760-767)

Bit value:
  - Bit = 1 if f32 > 0.0
  - Bit = 0 if f32 ≤ 0.0
```

**Rust Type Definition:**
```rust
#[repr(C, align(64))]
#[derive(Clone, Debug, PartialEq, Eq)]
pub struct QuantizedVector {
    data: [u8; 96],
}
```

**Alignment Verification:**
```rust
assert_eq!(std::mem::align_of::<QuantizedVector>(), 64);
assert_eq!(std::mem::size_of::<QuantizedVector>(), 96);
```

---

## SALVAGE POLICY

### Approved Functions from binary_semantic_cache v1.0 (MIT License)

**1. Hamming Distance Function**
- **Source:** `binary_semantic_cache/src/similarity.rs` lines 286-292
- **Function:** `hamming_distance_single`
- **License:** MIT
- **Status:** ✅ APPROVED for salvage

**Attribution Template:**
```rust
// Adapted from binary_semantic_cache v1.0 (MIT License)
// Copyright (c) 2024 Matteo Panzeri
// Original: https://github.com/[user]/binary_semantic_cache/blob/main/src/similarity.rs#L286-L292
```

**2. Binarization Function**
- **Source:** `binary_semantic_cache/src/quantization/encoder.rs` lines 343-359
- **Function:** `binarize_and_pack_single`
- **License:** MIT
- **Status:** ✅ APPROVED for salvage

**Attribution Template:**
```rust
// Adapted from binary_semantic_cache v1.0 (MIT License)
// Copyright (c) 2024 Matteo Panzeri
// Original: https://github.com/[user]/binary_semantic_cache/blob/main/src/quantization/encoder.rs#L343-L359
```

**Everything Else:** Must be written from scratch.

---

## PERFORMANCE TARGETS

### Quantization Latency
- **Target:** <1ms per vector (mean)
- **Verification:** `cargo bench bench_quantize`
- **Acceptable:** <2ms (if target not met, document and defer optimization to W9.3)

### Hamming Distance Latency
- **Target:** <50 CPU cycles per comparison (x86_64 AVX2)
- **Verification:** `cargo bench bench_hamming_distance`
- **Calculation:** (ns × CPU_GHz) / 1000
  - Example: 15ns @ 3.0GHz = 45 cycles ✓
  - Example: 20ns @ 3.0GHz = 60 cycles ✗ (defer SIMD to W9.3)
- **Acceptable:** <200 cycles (baseline without SIMD)

### Memory Alignment
- **Target:** 64-byte alignment for SIMD
- **Verification:** `std::mem::align_of::<QuantizedVector>() == 64`
- **Required:** Use `#[repr(C, align(64))]` attribute

### Coverage
- **Target:** 100% of public API
- **Verification:** `cargo tarpaulin --packages edgevec`
- **Required:** ≥100% (no exceptions)

---

## INTEGRATION POINTS

### HNSW Integration
**Where:** `src/hnsw/index.rs` (search function)
**How:** Replace f32 distance computation with Hamming distance
**Note:** Do NOT modify HNSW in W8.1 (integration is W8.3a)

### Vector Storage Integration
**Where:** `src/storage/vector_store.rs`
**How:** Store QuantizedVector instead of Vec<f32>
**Note:** Do NOT modify storage in W8.1 (integration is W8.3b)

### WASM Exports
**Where:** `src/wasm/bindings.rs`
**How:** Export quantization functions to JavaScript
**Note:** Do NOT modify WASM in W8.1 (integration is W8.3c)

---

## EXISTING CODEBASE STRUCTURE (Read Before Creating Modules)

### Current Structure (from Week 7):
```
src/
├── lib.rs                    # Crate root
├── hnsw/
│   ├── mod.rs
│   ├── index.rs             # HnswIndex implementation
│   └── neighbor.rs          # Neighbor pool
├── storage/
│   ├── mod.rs
│   ├── vector_store.rs      # Vector storage
│   └── metadata.rs
├── distance/
│   ├── mod.rs
│   └── metrics.rs           # L2, cosine, dot product
└── wasm/
    ├── mod.rs
    └── bindings.rs
```

### Where to Add Quantization:
```
src/quantization/            # NEW MODULE (create in W8.1)
├── mod.rs                   # Public API
└── binary.rs                # Binary quantization
```

### Update `src/lib.rs`:
```rust
pub mod quantization;  // Add this line
```

---

## COMMON PITFALLS

### 1. Endianness Issues
**Problem:** Bit packing order differs across platforms
**Solution:** Use explicit little-endian packing (document in code)

### 2. Alignment Not Enforced
**Problem:** SIMD requires 64-byte alignment, Rust default is 8 bytes
**Solution:** Use `#[repr(C, align(64))]` on QuantizedVector

### 3. Floating-Point Edge Cases
**Problem:** NaN, Inf, -0.0 have undefined behavior in comparison
**Solution:** Document behavior: NaN and Inf both map to 0 bit

### 4. Off-by-One in Bit Packing
**Problem:** Confusing byte index vs bit index
**Solution:** Use `byte_idx = bit_idx / 8` and `bit_offset = bit_idx % 8`

### 5. Unsafe Code Without Proof
**Problem:** Using `unsafe` for performance without safety proof
**Solution:** Either avoid `unsafe` or document SAFETY proof for every use

---

## TESTING STRATEGY

### Unit Tests (tests/unit/test_quantization.rs)
1. **Correctness Tests:**
   - Zero vector → all bits 0
   - Positive vector → all bits 1
   - Mixed vector → correct bit pattern
   - Hamming distance: identical → 0, opposite → 768

2. **Property Tests:**
   - Determinism: `quantize(v) == quantize(v)`
   - Symmetry: `hamming(a,b) == hamming(b,a)`
   - Triangle inequality (not applicable for Hamming)

3. **Edge Case Tests:**
   - NaN handling
   - Inf handling
   - -0.0 handling
   - Empty vector (should panic or return error)

4. **Performance Tests:**
   - Alignment verification
   - Benchmark baseline

### Integration Tests (W8.3a-c, Day 2-3)
- HNSW search with quantized vectors
- Persistence round-trip
- WASM interop

---

## REFERENCE IMPLEMENTATIONS

### Hamming Distance (Portable)
```rust
pub fn hamming_distance(a: &[u8; 96], b: &[u8; 96]) -> u32 {
    let mut distance = 0u32;
    for i in 0..96 {
        let xor = a[i] ^ b[i];
        distance += xor.count_ones();
    }
    distance
}
```

### Hamming Distance (SIMD - Future W9.3)
```rust
#[cfg(target_feature = "avx2")]
pub fn hamming_distance_simd(a: &[u8; 96], b: &[u8; 96]) -> u32 {
    use std::arch::x86_64::*;
    unsafe {
        // Use _mm256_popcnt_epi64 for 4x speedup
        // Implementation deferred to W9.3
    }
}
```

---

## NEXT STEPS AFTER W8.1

**Day 2 (W8.2):** Fuzzing
- TEST_ENGINEER uses quantization code to create fuzz targets
- 100k iterations, edge case discovery

**Day 3 (W8.3a):** E2E Insert-Search
- RUST_ENGINEER integrates quantization into HNSW search
- Verifies recall ≥0.95 with quantized vectors

**Day 4 (W8.4a):** Performance Budgets
- BENCHMARK_SCIENTIST defines acceptable latency/memory budgets
- Measures quantization overhead

**Day 5 (W8.4b):** Profiling
- BENCHMARK_SCIENTIST identifies hot paths
- Prepares optimization plan for W9.3

---

**END OF CONTEXT BUNDLE**

**Next:** Proceed to implementation using `01_W8.1_RUST_ENGINEER.md`
