# W5D25 - Comprehensive Benchmarking

## Goal
Validate performance against the "1M Vector" target via extrapolation.

## Specifications
- **Dataset:** Synthetic vectors (Random, 1536 dim).
- **Sizes:** 10k, 50k, 100k, 200k (if RAM allows in dev env).
- **Metrics:**
  - Build Time
  - Search Latency (P50, P95, P99)
  - Memory Usage (Bytes/Vector)

## Tasks
1. [x] Create `benches/scaling_benchmark.rs`.
2. [x] Run benchmark suite (Partial run due to timeout on >50k).
3. [x] Plot results (Excel/Python) and fit curve.
4. [x] Extrapolate to 1M vectors.
5. [x] Generate report: `docs/benchmarks/W5_scaling_report.md`.

## Acceptance
- Extrapolated Search Latency < 10ms (Native).
- Extrapolated Memory < 1GB for 1M vectors (assuming quantized/compressed, or raw float32 if budget allows). *Note: 1M * 1536 * 4 bytes = 6GB. We need quantization/compression for 1M scale on small devices, but that's a later milestone. For now, report raw usage.*

## Notes
- **Optimization Issue:** Build times for N>50k are excessive due to `opt-level="z"` preventing SIMD optimizations.
- **Search Performance:** Excellent (~2ms P99 for 100k extrapolated).
